# Finjustere språkmodeller på Nav.no

I denne notatboken skal vi illustrere hvordan man kan komme i gang med
finjustering av språkmodeller. Vi skal gå gjennom stegene for å gjøre klart et
datasett basert på data på [datamarkedsplassen](https://data.ansatt.nav.no/).
Hvordan teste ytelsen på en embeddingmodell på dette datasettet. Og tilslutt,
skal vi se hvordan vi kan forbedre ytelsen med finjustering.

## Prosjektoppsett

Vi anbefaler at man bruker [`uv`](https://docs.astral.sh/uv/) for å opprette
prosjekt og styre avhengigheter.

La oss starte med å lage et prosjekt:

```bash
uv init --app --python 3.12 navno_finetune
```

Inne i prosjektet kan vi fjerne `main.py` (eller `hello.py` avhengig av din
versjon av `uv`). Hvis du ønsker å følge denne veiledningen kan man enkelt
opprettet en Jupyter Notebook fil og klippe og lime kode fra veiledningen.
Alternativt kan man strukturere kode etter eget ønske og bruke veiledningen til
inspirasjon.

::: {.callout collapse="true"}
## Gjenbruke _denne_ veiledningen

Denne veiledningen er også mulig å gjenbruke ordrett. For å være enkelt å
publisere er den strukturert som et Quarto prosjekt, men det er fullt mulig å
klone veiledningen og bare kjøre Quarto filen direkte.
:::

# Datasett

Før vi kan starte å finjustere trenger vi et datasett vi kan teste på og som vi
kan bruke til trening. Vi kommer til å benytte [innholdet på
Nav.no](https://data.ansatt.nav.no/dataproduct/6c7327e2-5894-4423-b6b2-52affa3f5b29/Innhold%20p%C3%A5%20Nav.no/7993897c-9fd4-46ee-86dd-5001621a2695)
som utgangspunkt.

## Laste ned rådata

La oss starte med å laste ned rådata fra BigQuery. For å gjøre dette kommer vi
til å bruke `google-cloud-bigquery` og [Polars](https://docs.pola.rs/).

::: {.callout-note}
## Nødvendige avhengigheter

For å installere avhengigheter kjører vi følgende `uv` kommandoer:

```bash
uv add google-cloud-bigquery
uv add polars --extra pyarrow
```
:::

Vi starter med å hente all rådata og opprette en Polars `DataFrame`.

```{python}
import polars as pl
from google.cloud import bigquery

client = bigquery.Client()

# Bygg opp spørring og hent all data for gitt tidspunkt
QUERY = (
    'SELECT * FROM `nks-aiautomatisering-prod-194a.navno_crawl.navno` '
    'WHERE DATE(crawl_date) = DATE(2025, 02, 25)')
query_job = client.query(QUERY)
rows = query_job.result()  # Vent på nedlasting

df = pl.from_arrow(rows.to_arrow())  # Opprett dataframe med rådata
```

La oss inspisere dataene, før vi konverterer det til et mer passende format for
språkmodeller.

```{python}
# | column: page
df.head()
```

## Strukturere data for språkmodeller

For å finjustere en embeddingmodell er det i hovedsak _fire_ ulike måter å
strukturere et datasett:

- **Positive pair**: Et par setninger som er relatert (f.eks `(spørsmål,
svar)`).
- **Triplets**: Likt som _positive pair_, men med et anti-relatert element.
    - Fordi vi kan bruke treningsfunksjoner (_loss_-funksjon) som kan gjennbruke
    data i _positiv pair_ datasettet er ikke dette formatet like mye brukt.
- **Pair with Similarity Score**: Et par setninger og en verdi som representerer
hvor like disse setningene er.
- **Text with Classes**: En setning med tilhørende klasse. Kan konverteres til
andre formater over.

::: {.column-margin}
Hentet fra [SBERT.net - Dataset
Overview](https://sbert.net/docs/sentence_transformer/dataset_overview.html).
:::

Basert på dataene over så er det naturlig å velge _Positiv Pair_. Dette er fordi
vi kan koble flere kolonner sammen for å lage disse parene. Vi kan for eksempel
koble tittel og innhold sammen, noe som burde forsterke koblingen mellom tittel
og relevant innhold for språkmodellen.

La oss starte med det åpenbare `(tittel, innhold)` paret

```{python}
df_title_content = df.select(
    pl.col("display_name").alias("anchor"),
    pl.col("content").alias("positiv")
)
df_title_content.head()
```

En annen åpenbar kobling er `(tittel, ingress)`

```{python}
df_title_ingres = df.filter(
    pl.col("ingress").str.len_bytes() > 0).select(
        pl.col("display_name").alias("anchor"),
        pl.col("ingress").alias("positiv")
    ).unique()
df_title_ingres.head()
```

La oss også koble side tittel sammen med titler til innholdet og innhold

```{python}
df_all_titles_content = df.filter(pl.col("headers").list.len() > 0).select(
    (pl.col("display_name") + "\n" + pl.col("headers").list.join(separator="\n")).alias("anchor"),
    pl.col("content").alias("positiv")
)
df_all_titles_content.head()
```

La oss koble alle disse tabellene sammen og for å lage et endelig datasett.

```{python}
df_train = pl.concat([df_title_content, df_title_ingres, df_all_titles_content])
df_train.describe()
```

## Rense datasett

Før vi sier oss fornøyd skal vi vaske dataene våre litt. I tabellene over har du
kanskje lagt merke til at flere av kolonnene inneholder HTML elementer. Det er
ikke i utgangspunktet noe galt å bruke dette for trening, men siden vi her
fokuserer på en embeddingmodell ønsker vi at den fokuserer på semantikken og
ikke formatet. Vi skal derfor prøve å renske bort alle HTML tag-er^[Vi gjør det
enkelt med en `regex` inspirert av
[StackOverflow](https://stackoverflow.com/a/12982689).].

```{python}
df_train = df_train.select(
    pl.col("anchor").str.replace_all("<.*?>", " ").str.strip_chars(),
    pl.col("positiv").str.replace_all("<.*?>", " ").str.strip_chars(),
)
df_train.head()
```

## Lagre til fil

La oss avslutte med å lagre data til en fil slik at vi enkelt kan gjenskape
treningen og samtidig dele data med andre på en enkel måte. Et format som kan
være praktisk er [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) som
både er effektivt for å lagre dataframe data og samtidig er godt støttet i de
fleste verktøy vi bruker i Nav.

```{python}
df_train.write_parquet("dataset.parquet")
```
