{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finjustere spr친kmodeller p친 Nav.no\n",
    "\n",
    "I denne notatboken skal vi illustrere hvordan man kan komme i gang med\n",
    "finjustering av spr친kmodeller. Vi skal g친 gjennom stegene for 친 gj칮re klart et\n",
    "datasett basert p친 data p친 [datamarkedsplassen](https://data.ansatt.nav.no/).\n",
    "Hvordan teste ytelsen p친 en embeddingmodell p친 dette datasettet. Og tilslutt,\n",
    "skal vi se hvordan vi kan forbedre ytelsen med finjustering.\n",
    "\n",
    "## Prosjektoppsett\n",
    "\n",
    "Vi anbefaler at man bruker [`uv`](https://docs.astral.sh/uv/) for 친 opprette\n",
    "prosjekt og styre avhengigheter.\n",
    "\n",
    "La oss starte med 친 lage et prosjekt:\n",
    "\n",
    "```bash\n",
    "uv init --app --python 3.12 navno_finetune\n",
    "```\n",
    "\n",
    "Inne i prosjektet kan vi fjerne `main.py` (eller `hello.py` avhengig av din\n",
    "versjon av `uv`). Hvis du 칮nsker 친 f칮lge denne veiledningen kan man enkelt\n",
    "opprettet en Jupyter Notebook fil og klippe og lime kode fra veiledningen.\n",
    "Alternativt kan man strukturere kode etter eget 칮nske og bruke veiledningen til\n",
    "inspirasjon.\n",
    "\n",
    "::: {.callout-tip collapse=\"true\"}\n",
    "## Tilgang til _denne_ notatboken\n",
    "\n",
    "Du kan laste ned notatboken denne datafortellingen er basert p친, [p친\n",
    "Github](https://github.com/navikt/navno_finetune/blob/main/index.ipynb).\n",
    ":::\n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## Valgfrie avhengigheter\n",
    "\n",
    "For 친 gj칮re livet litt mer fargerikt installerer vi `rich`, dette er ikke\n",
    "n칮dvendig, men vil gi penere utskrift.\n",
    "\n",
    "```bash\n",
    "uv add rich\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pr칮v 친 bruke `rich.print` som standard hvis tilgjengelig\n",
    "try:\n",
    "    from rich import print\n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Datasett\n",
    "\n",
    "F칮r vi kan starte 친 finjustere trenger vi et datasett vi kan teste p친 og som vi\n",
    "kan bruke til trening. Vi kommer til 친 benytte [innholdet p친\n",
    "Nav.no](https://data.ansatt.nav.no/dataproduct/6c7327e2-5894-4423-b6b2-52affa3f5b29/Innhold%20p%C3%A5%20Nav.no/7993897c-9fd4-46ee-86dd-5001621a2695)\n",
    "som utgangspunkt.\n",
    "\n",
    "### Laste ned r친data\n",
    "\n",
    "La oss starte med 친 laste ned r친data fra BigQuery. For 친 gj칮re dette kommer vi\n",
    "til 친 bruke `google-cloud-bigquery` og [Polars](https://docs.pola.rs/).\n",
    "\n",
    "::: {.callout-note}\n",
    "## N칮dvendige avhengigheter\n",
    "\n",
    "For 친 installere avhengigheter kj칮rer vi f칮lgende `uv` kommandoer:\n",
    "\n",
    "```bash\n",
    "uv add google-cloud-bigquery\n",
    "uv add polars --extra pyarrow\n",
    "```\n",
    ":::\n",
    "\n",
    "Vi starter med 친 hente all r친data og opprette en Polars `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Bygg opp sp칮rring og hent all data for gitt tidspunkt\n",
    "QUERY = (\n",
    "    \"SELECT * FROM `nks-aiautomatisering-prod-194a.navno_crawl.navno` \"\n",
    "    \"WHERE DATE(crawl_date) = DATE(2025, 02, 25)\"\n",
    ")\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()  # Vent p친 nedlasting\n",
    "\n",
    "df = pl.from_arrow(rows.to_arrow())  # Opprett dataframe med r친data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss inspisere dataene, f칮r vi konverterer det til et mer passende format for\n",
    "spr친kmodeller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | column: page\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strukturere data for spr친kmodeller\n",
    "\n",
    "For 친 finjustere en embeddingmodell er det i hovedsak _fire_ ulike m친ter 친\n",
    "strukturere et datasett:\n",
    "\n",
    "- **Positive pair**: Et par setninger som er relatert (f.eks `(sp칮rsm친l,\n",
    "svar)`).\n",
    "- **Triplets**: Likt som _positive pair_, men med et anti-relatert element.\n",
    "    - Fordi vi kan bruke treningsfunksjoner (_loss_-funksjon) som kan gjennbruke\n",
    "    data i _positiv pair_ datasettet er ikke dette formatet like mye brukt.\n",
    "- **Pair with Similarity Score**: Et par setninger og en verdi som representerer\n",
    "hvor like disse setningene er.\n",
    "- **Text with Classes**: En setning med tilh칮rende klasse. Kan konverteres til\n",
    "andre formater over.\n",
    "\n",
    "::: {.column-margin}\n",
    "Hentet fra [SBERT.net - Dataset\n",
    "Overview](https://sbert.net/docs/sentence_transformer/dataset_overview.html).\n",
    ":::\n",
    "\n",
    "Basert p친 dataene over s친 er det naturlig 친 velge _Positiv Pair_. Dette er fordi\n",
    "vi kan koble flere kolonner sammen for 친 lage disse parene. Vi kan for eksempel\n",
    "koble tittel og innhold sammen, noe som burde forsterke koblingen mellom tittel\n",
    "og relevant innhold for spr친kmodellen.\n",
    "\n",
    "::: {.callout-caution appearance=\"minimal\"}\n",
    "Vi legger ogs친 ved `path` slik at rader kan kobles sammen mot samme sti, dette\n",
    "kommer vi til 친 bruke for 친 markere relevante dokumenter.\n",
    ":::\n",
    "\n",
    "La oss starte med det 친penbare `(tittel, innhold)` paret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_titles_content = df.select(\n",
    "    pl.col(\"path\"),\n",
    "    (pl.col(\"display_name\") + \"\\n\" + pl.col(\"headers\").list.join(separator=\"\\n\")).alias(\n",
    "        \"anchor\"\n",
    "    ),\n",
    "    pl.col(\"content\").alias(\"positive\"),\n",
    ")\n",
    "df_all_titles_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En annen 친penbar kobling er `(tittel, ingress)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title_ingres = (\n",
    "    df.filter(pl.col(\"ingress\").str.len_bytes() > 0)\n",
    "    .select(\n",
    "        pl.col(\"path\"),\n",
    "        pl.col(\"display_name\").alias(\"anchor\"),\n",
    "        pl.col(\"ingress\").alias(\"positive\"),\n",
    "    )\n",
    "    .unique()\n",
    ")\n",
    "df_title_ingres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss koble alle disse tabellene sammen og for 친 lage et endelig datasett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pl.concat([df_title_ingres, df_all_titles_content])\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rense datasett\n",
    "\n",
    "F칮r vi sier oss forn칮yd skal vi vaske dataene v친re litt. I tabellene over har du\n",
    "kanskje lagt merke til at flere av kolonnene inneholder HTML elementer. Det er\n",
    "ikke i utgangspunktet noe galt 친 bruke dette for trening, men siden vi her\n",
    "fokuserer p친 en embeddingmodell 칮nsker vi at den fokuserer p친 semantikken og\n",
    "ikke formatet. Vi skal derfor pr칮ve 친 renske bort alle HTML tag-er^[Vi gj칮r det\n",
    "enkelt med en `regex` inspirert av\n",
    "[StackOverflow](https://stackoverflow.com/a/12982689).]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "    pl.col(\"anchor\").str.replace_all(\"<.*?>\", \" \").str.strip_chars(),\n",
    "    pl.col(\"positive\").str.replace_all(\"<.*?>\", \" \").str.strip_chars(),\n",
    ")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagre til fil\n",
    "\n",
    "La oss avslutte med 친 lagre data til en fil slik at vi enkelt kan gjenskape\n",
    "treningen og samtidig dele data med andre p친 en enkel m친te. Et format som kan\n",
    "v칝re praktisk er [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) som\n",
    "b친de er effektivt for 친 lagre dataframe data og samtidig er godt st칮ttet i de\n",
    "fleste verkt칮y vi bruker i Nav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.write_parquet(\"dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "::: {.callout-tip}\n",
    "## Overgang til `datasets`\n",
    "\n",
    "Avhengig av dine preferanser s친 er dette et naturlig tidspunkt 친 g친 over til\n",
    "`datasets`. `datasets` er et bibliotek for datasett som er veldig mye brukt med\n",
    "spr친kmodeller og 游뱅 Hugging Face. Vi kommer til 친 laste inn `datasets` litt\n",
    "senere da treningsmetoden vi skal benytte bruker dette biblioteket.\n",
    "\n",
    "I denne datafortellingen kommer vi til 친 holde oss til Polars s친 lenge som mulig\n",
    "p친 grunn av tidligere kjennskap til dette biblioteket samt at vi fortsatt\n",
    "trenger noen operasjoner som vil v칝re raskere i Polars enn i `datasets`.\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening og test\n",
    "\n",
    "N친 som vi har laget et fullstendig treningssett kan vi begynne 친 tenke p친 친 dele\n",
    "opp i en trenings del og en test del. Dette gj칮r vi for 친 ha en del som modellen\n",
    "f친r lov til 친 se p친, trenings delen, og en del som er helt ny for modellen, test\n",
    "delen. Ved 친 skille slik f친r vi mulighet til 친 evaluere hvor godt modellen\n",
    "fungerer p친 ting den ikke har sett f칮r.\n",
    "\n",
    "Vi starter med 친 legge til en `ID` kolonne p친 datasettet v친rt slik at vi kan\n",
    "unikt identifisere rader, dette kommer vi til 친 trenge senere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df_train.with_row_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter deler vi datasettet i en del for trening, meste parten, og en del for\n",
    "testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Litt komplisert 친 lage trening/test split i Polars\n",
    "#\n",
    "# Vi starter med 친 randomisere hele datasettet\n",
    "dataset = dataset.sample(fraction=1, shuffle=True, seed=12345)\n",
    "# Beregne antall rader vi skal bruke\n",
    "num_test = int(0.2 * len(dataset))\n",
    "# Deretter ta de f칮rste `num_test` radene til test\n",
    "test_dataset = dataset.head(num_test)\n",
    "# Tilslutt tar vi alle utenom de f칮rste `num_test` radene til trening\n",
    "train_dataset = dataset.tail(-num_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 친 kunne reprodusere eksperimentene p친 andre maskiner lagrer vi ogs친 trening\n",
    "og test data som egne filer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset.write_parquet(\"test_dataset.parquet\")\n",
    "train_dataset.write_parquet(\"train_dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi legger her til innlesning p친 nytt slik at vi alltid kan starte friskt her\n",
    "uten 친 m친tte kj칮re alle celler over."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "# Les inn datasett\n",
    "test_dataset = pl.read_parquet(\"test_dataset.parquet\")\n",
    "train_dataset = pl.read_parquet(\"train_dataset.parquet\")\n",
    "# Kombiner for 친 kunne arbeide med hele datasettet\n",
    "dataset = pl.concat([test_dataset, train_dataset])\n",
    "# Skriv ut raske tall\n",
    "print(f\"Antall elementer [bold magenta]totalt[/]:\\t{len(dataset)}\")\n",
    "print(f\"Antall elementer i [bold green]trening[/]:\\t{len(train_dataset)}\")\n",
    "print(f\"Antall elementer i [bold blue]test[/]:\\t{len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "N친 som vi har opprettet et datasett kan vi bruke dette for 친 lage oss et corpus\n",
    "친 trene p친/med.\n",
    "\n",
    "Vi starter med 친 lage oss et sett med alt innhold, \"corpus\", og et sett med\n",
    "\"queries\" (de elementene som vi 칮nsker 친 teste mot innholdet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merk at vi bruker `dataset` for 친 bruke _alt_ innhold\n",
    "corpus = dict(dataset.select([\"id\", \"positive\"]).rows())\n",
    "# For \"queries\" bruker vi det vi har plukket ut i test\n",
    "queries = dict(test_dataset.select([\"id\", \"anchor\"]).rows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi trenger s친 친 lage oss en mapping mellom \"queries\" og relevant innhold. I v친rt\n",
    "tilfellet s친 vil det v칝re overlapp for alle deler som kommer fra samme sti p친\n",
    "Nav.no. Sagt p친 en annen m친te, vi 칮nsker 친 vekte elementer fra samme sti h칮yere\n",
    "under evaluering slik at et element fra `/dagpenger` er viktigere eller bedre\n",
    "enn et element fra `/sykepenger` hvis s칮ket handler om Dagpenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = {}\n",
    "for qid in queries.keys():\n",
    "    navno_path = (\n",
    "        dataset.filter(pl.col(\"id\") == qid).unique(\"path\").item(row=0, column=\"path\")\n",
    "    )\n",
    "    relevant_docs[qid] = set(\n",
    "        dataset.filter(pl.col(\"path\") == navno_path).get_column(\"id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spr친kmodell\n",
    "\n",
    "N친 som vi har ordnet oss med litt data er det endelig p친 tide 친 velge en\n",
    "spr친kmodell. Vi kommer til 친 bruke\n",
    "[`sentence-transformers`](https://sbert.net/index.html) for modellen og trening\n",
    "s친 la oss f칮rst ordne n칮dvendige pakker.\n",
    "\n",
    ":::: {.callout-note}\n",
    "## N칮dvendige avhengigheter\n",
    "\n",
    "Vi trenger et par pakker for `sentence-transformers` og de avhenger av riktig\n",
    "oppsett for effektiv trening.\n",
    "\n",
    "::: {.panel-tabset}\n",
    "## Uten CUDA (Linux, Mac og Windows uten GPU)\n",
    "For maskiner uten dedikert Nvidia GPU kan man enkelt installere som f칮lger:\n",
    "\n",
    "```bash\n",
    "uv add transformers --extra torch\n",
    "uv add sentence-transformers --extra train\n",
    "```\n",
    "\n",
    "## CUDA\n",
    "\n",
    "Hvis du har et dedikert grafikkort kan du tjene mye p친 친 installere PyTorch med\n",
    "CUDA st칮tte.\n",
    "\n",
    "Her anbefaler vi 친 f칮lge oppskriften p친 [PyTorch sin\n",
    "hjemmeside](https://pytorch.org/get-started/locally/) for 친 f친 riktig oppsett\n",
    "for akkurat din maskin.\n",
    "\n",
    "Deretter trenger du:\n",
    "```bash\n",
    "uv add transformers\n",
    "uv add sentence-transformers --extra train\n",
    "```\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "N친r det kommer til valg av spr친kmodell s친 er det vanskelig 친 gi noen konkrete\n",
    "anbefalinger, nettopp fordi man kan tilpasse modellene til egne data slik vi\n",
    "gj칮r her. En god oversikt over hvordan 친 velge spr친kmodell finnes i [Nav sin\n",
    "tekniske\n",
    "veileder](https://data.ansatt.nav.no/quarto/b9ec1385-d596-47e2-a3d2-8cbc85c577a3/_book/llm/lokale.html).\n",
    "\n",
    "Vi kommer til 친 g친 videre med\n",
    "[`Alibaba-NLP/gte-modernbert-base`](https://huggingface.co/Alibaba-NLP/gte-modernbert-base).\n",
    "Denne har vi valgt av f칮lgende grunner:\n",
    "\n",
    "- Den gj칮r det godt i sammenligninger mot embeddingmodeller av tilsvarende st칮rrelse\n",
    "- Det er en relativt liten, $149$ millioner parametere, modell som burde passe\n",
    "fint p친 en laptop\n",
    "- Den har et stort kontekstvindu p친 $8192$ token\n",
    "    - Noe som betyr at den kan jobbe med st칮rre sammenhengende tekster\n",
    "- Den er lisensiert p친 en m친te som gj칮r at vi enkelt kan ta den i bruk i Nav\n",
    "(`Apache 2.0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"Alibaba-NLP/gte-modernbert-base\",\n",
    "    # NOTE: Vi velger `device` spesifikt tilpasset Mac, her burde man bruke\n",
    "    # \"cuda\" hvis tilgjengelig, eller annet tilpasset din maskin\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    "    # NOTE: Vi setter `padding` til `max_length` for 친 pr칮ve 친 unng친 en\n",
    "    # minnelekasje p친 Mac som f칮rer til at trening og evaluering bruker for mye\n",
    "    # minne. For annen maskinvare kan dette valget fjernes!\n",
    "    tokenizer_kwargs=dict(padding=\"max_length\"),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluere spr친kmodell\n",
    "\n",
    "La oss n친 se litt p친 hvordan modellen v친r gj칮r det p친 datasettet v친rt.\n",
    "\n",
    "Vi m친 starte med 친 definere en m친te 친 evaluere modellen v친r, her har ogs친\n",
    "[`sentence-transformers` god\n",
    "st칮tte](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator)\n",
    "s친 vi benytter det som er innebygget der."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=\"modernbert\",\n",
    "    score_functions={\"cosine\": cos_sim},\n",
    "    batch_size=4,  # Skru denne ned eller opp avhengig av tilgjengelig minne, h칮yere gir raskere evaluering\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter kan vi benytte `evaluator` til 친 vurdere modellene v친r."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "base_eval = evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fra evalueringen over er det kanskje mest interessant 친 se p친\n",
    "[NDCG@10](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) som sier oss\n",
    "noe om kvaliteten p친 rangering av treff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rangeringskvalitet (NDCG@10): {base_eval['modernbert_cosine_ndcg@10']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finjustering\n",
    "\n",
    "Etter at vi n친 har valgt og testet en embeddingmodell er det n친 p친 tide 친 se om\n",
    "vi kan forbedre ytelsen til modellen ved 친 finjustere.\n",
    "\n",
    "Vi kommer til 친 holde oss i `sentence-transformers` verden og benytte loss\n",
    "funksjon og treningsmetoder derfra.\n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## Valgfrie avhengigheter\n",
    "\n",
    "For 친 칮ke hastighet p친 treningen kan det v칝re lurt 친 installere `flash-attn` som\n",
    "er en optimalisert versjon av Flash Attention. Dette vil gj칮re finjusteringen\n",
    "raskere p친 st칮ttet maskinvare (for det meste GPU-er).\n",
    "\n",
    "```bash\n",
    "uv add flash-attn\n",
    "```\n",
    ":::\n",
    "\n",
    "### Treningsmetode (loss function)\n",
    "\n",
    "F칮r vi kan finjustere spr친kmodellen v친r m친 vi definere en treningsmetode som\n",
    "forteller systemet hvor bra, eller d친rlig, modellen v친r gj칮r det n친r vi\n",
    "presenterer den for eksempler.\n",
    "\n",
    "::: {.column-margin}\n",
    "![Illustrasjon av hvordan `MultipleNegativeRankingLoss` optimaliserer ved 친\n",
    "knytte \"n친v칝rende\" treningseksempel tettere og samtidig \"skyve bort\" alle andre\n",
    "eksempler](./assets/MultipleNegativeRankingLoss.png)\n",
    ":::\n",
    "\n",
    "For datasett av typen _Positiv Pair_ er\n",
    "[`MultipleNegativesRankingLoss`](https://sbert.net/docs/package_reference/sentence_transformer/losses.html#multiplenegativesrankingloss)\n",
    "veldig passende fordi den kan gjenbruke \"alle andre\" eksempler i treningssettet\n",
    "som negative eksempler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.losses import MultipleNegativesRankingLoss\n",
    "\n",
    "train_loss = MultipleNegativesRankingLoss(model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Treningsoppsett\n",
    "\n",
    "Etter at vi har definert en treningsmetode s친 m친 vi gj칮re litt husarbeid for 친\n",
    "definere hvordan trening skal foreg친.\n",
    "\n",
    "::: {.callout-note}\n",
    "## N칮dvendige avhengigheter\n",
    "\n",
    "Siden `sentence-transformers` bruker `datasets` for 친 strukturere\n",
    "treningseksempler trenger vi ogs친 dette biblioteket.\n",
    "\n",
    "```bash\n",
    "uv add datasets\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainingArguments\n",
    "from sentence_transformers.training_args import BatchSamplers\n",
    "\n",
    "# Definer hvordan trening skal foreg친\n",
    "train_args = SentenceTransformerTrainingArguments(\n",
    "    output_dir=\"gte-modernbert-navno\",\n",
    "    num_train_epochs=4,  # Antall epoker 친 trene, flere er bedre\n",
    "    per_device_train_batch_size=4,  # Bestemt av maskinvare, h칮yere trener raskere\n",
    "    per_device_eval_batch_size=4,\n",
    "    warmup_ratio=0.1,\n",
    "    learning_rate=2e-5,\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    tf32=None,  # Kjekt 친 sette til `True` hvis maskinvare st칮tter (krever nyere Nvidia GPU)\n",
    "    fp16=False,  # Sett til `True` hvis man ikke kan bruke `bf16`\n",
    "    bf16=True,  # Kjekt 친 sette p친 hvis maskinvare st칮tter (st칮ttes av Mac og Nvidia GPU-er)\n",
    "    batch_sampler=BatchSamplers.NO_DUPLICATES,  # Veldig praktisk 친 fjerne duplikater n친r man har Positiv Pair\n",
    "    eval_strategy=\"steps\",  # Evaluer etter hver X steg\n",
    "    save_strategy=\"steps\",  # Lagre modell etter X steg\n",
    "    save_steps=50,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=3,  # Bare spar p친 de 3 siste modellene\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi konverterer s친 treningsdataene v친re til et `datasets` slik at det er\n",
    "kompatibelt med `sentence-transformers`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_ds = Dataset.from_polars(train_dataset.select([\"anchor\", \"positive\"]))\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ogs친 kan vi sette opp treningsregimet v친rt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformerTrainer\n",
    "\n",
    "trainer = SentenceTransformerTrainer(\n",
    "    model=model,\n",
    "    args=train_args,\n",
    "    train_dataset=train_ds,\n",
    "    loss=train_loss,\n",
    "    evaluator=evaluator,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utf칮re finjustering\n",
    "\n",
    "Med det unnagjort kan vi bare kj칮re treningsregimet v친rt for 친 f친 en finjustert\n",
    "modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "\n",
    "# Utf칮r trening\n",
    "trainer.train()\n",
    "\n",
    "# Pass p친 at vi lagrer modellen\n",
    "trainer.save_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluer finjustert modell\n",
    "\n",
    "N친 som vi har finjustert modellen gjenst친r det bare 친 evaluere om finjusteringen\n",
    "hadde noe for seg.\n",
    "\n",
    "Vi kan gjenbruke evalueringen vi brukte tidligere, men det er lurt 친 laste\n",
    "modellen p친 nytt slik at vi er sikker p친 at vi evaluerer riktig modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    train_args.output_dir,  # NOTE: Vi bytter ut modellnavn her med mappen hvor vi lagret finjustert modell\n",
    "    # Merk at her velger vi `device` spesifikt tilpasset Mac, her burde man\n",
    "    # bruke \"cuda\" hvis tilgjengelig, eller annet tilpasset din maskin\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "final_eval = evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss s친 se hvordan det gikk med finjustert modell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Ikke finjustert (NDCG@10):\\t{base_eval['modernbert_cosine_ndcg@10']}\")\n",
    "print(f\"Finjustert (NDCG@10):\\t{final_eval['modernbert_cosine_ndcg@10']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
