{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finjustere språkmodeller på Nav.no\n",
    "\n",
    "I denne notatboken skal vi illustrere hvordan man kan komme i gang med\n",
    "finjustering av språkmodeller. Vi skal gå gjennom stegene for å gjøre klart et\n",
    "datasett basert på data på [datamarkedsplassen](https://data.ansatt.nav.no/).\n",
    "Hvordan teste ytelsen på en embeddingmodell på dette datasettet. Og tilslutt,\n",
    "skal vi se hvordan vi kan forbedre ytelsen med finjustering.\n",
    "\n",
    "## Prosjektoppsett\n",
    "\n",
    "Vi anbefaler at man bruker [`uv`](https://docs.astral.sh/uv/) for å opprette\n",
    "prosjekt og styre avhengigheter.\n",
    "\n",
    "La oss starte med å lage et prosjekt:\n",
    "\n",
    "```bash\n",
    "uv init --app --python 3.12 navno_finetune\n",
    "```\n",
    "\n",
    "Inne i prosjektet kan vi fjerne `main.py` (eller `hello.py` avhengig av din\n",
    "versjon av `uv`). Hvis du ønsker å følge denne veiledningen kan man enkelt\n",
    "opprettet en Jupyter Notebook fil og klippe og lime kode fra veiledningen.\n",
    "Alternativt kan man strukturere kode etter eget ønske og bruke veiledningen til\n",
    "inspirasjon.\n",
    "\n",
    "::: {.callout-tip collapse=\"true\"}\n",
    "## Tilgang til _denne_ notatboken\n",
    "\n",
    "Du kan laste ned notatboken denne datafortellingen er basert på, [på\n",
    "Github](https://github.com/navikt/navno_finetune/blob/main/index.ipynb).\n",
    ":::\n",
    "\n",
    "::: {.callout-note collapse=\"true\"}\n",
    "## Valgfrie avhengigheter\n",
    "\n",
    "For å gjøre livet litt mer fargerikt installerer vi `rich`, dette er ikke\n",
    "nødvendig, men vil gi penere utskrift.\n",
    "\n",
    "```bash\n",
    "uv add rich\n",
    "```\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prøv å bruke `rich.print` som standard hvis tilgjengelig\n",
    "try:\n",
    "    from rich import print\n",
    "except ModuleNotFoundError:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Datasett\n",
    "\n",
    "Før vi kan starte å finjustere trenger vi et datasett vi kan teste på og som vi\n",
    "kan bruke til trening. Vi kommer til å benytte [innholdet på\n",
    "Nav.no](https://data.ansatt.nav.no/dataproduct/6c7327e2-5894-4423-b6b2-52affa3f5b29/Innhold%20p%C3%A5%20Nav.no/7993897c-9fd4-46ee-86dd-5001621a2695)\n",
    "som utgangspunkt.\n",
    "\n",
    "### Laste ned rådata\n",
    "\n",
    "La oss starte med å laste ned rådata fra BigQuery. For å gjøre dette kommer vi\n",
    "til å bruke `google-cloud-bigquery` og [Polars](https://docs.pola.rs/).\n",
    "\n",
    "::: {.callout-note}\n",
    "## Nødvendige avhengigheter\n",
    "\n",
    "For å installere avhengigheter kjører vi følgende `uv` kommandoer:\n",
    "\n",
    "```bash\n",
    "uv add google-cloud-bigquery\n",
    "uv add polars --extra pyarrow\n",
    "```\n",
    ":::\n",
    "\n",
    "Vi starter med å hente all rådata og opprette en Polars `DataFrame`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Bygg opp spørring og hent all data for gitt tidspunkt\n",
    "QUERY = (\n",
    "    \"SELECT * FROM `nks-aiautomatisering-prod-194a.navno_crawl.navno` \"\n",
    "    \"WHERE DATE(crawl_date) = DATE(2025, 02, 25)\"\n",
    ")\n",
    "query_job = client.query(QUERY)\n",
    "rows = query_job.result()  # Vent på nedlasting\n",
    "\n",
    "df = pl.from_arrow(rows.to_arrow())  # Opprett dataframe med rådata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss inspisere dataene, før vi konverterer det til et mer passende format for\n",
    "språkmodeller."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | column: page\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Strukturere data for språkmodeller\n",
    "\n",
    "For å finjustere en embeddingmodell er det i hovedsak _fire_ ulike måter å\n",
    "strukturere et datasett:\n",
    "\n",
    "- **Positive pair**: Et par setninger som er relatert (f.eks `(spørsmål,\n",
    "svar)`).\n",
    "- **Triplets**: Likt som _positive pair_, men med et anti-relatert element.\n",
    "    - Fordi vi kan bruke treningsfunksjoner (_loss_-funksjon) som kan gjennbruke\n",
    "    data i _positiv pair_ datasettet er ikke dette formatet like mye brukt.\n",
    "- **Pair with Similarity Score**: Et par setninger og en verdi som representerer\n",
    "hvor like disse setningene er.\n",
    "- **Text with Classes**: En setning med tilhørende klasse. Kan konverteres til\n",
    "andre formater over.\n",
    "\n",
    "::: {.column-margin}\n",
    "Hentet fra [SBERT.net - Dataset\n",
    "Overview](https://sbert.net/docs/sentence_transformer/dataset_overview.html).\n",
    ":::\n",
    "\n",
    "Basert på dataene over så er det naturlig å velge _Positiv Pair_. Dette er fordi\n",
    "vi kan koble flere kolonner sammen for å lage disse parene. Vi kan for eksempel\n",
    "koble tittel og innhold sammen, noe som burde forsterke koblingen mellom tittel\n",
    "og relevant innhold for språkmodellen.\n",
    "\n",
    "::: {.callout-caution appearance=\"minimal\"}\n",
    "Vi legger også ved `path` slik at rader kan kobles sammen mot samme sti, dette\n",
    "kommer vi til å bruke for å markere relevante dokumenter.\n",
    ":::\n",
    "\n",
    "La oss starte med det åpenbare `(tittel, innhold)` paret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_titles_content = df.select(\n",
    "    pl.col(\"path\"),\n",
    "    (pl.col(\"display_name\") + \"\\n\" + pl.col(\"headers\").list.join(separator=\"\\n\")).alias(\n",
    "        \"anchor\"\n",
    "    ),\n",
    "    pl.col(\"content\").alias(\"positiv\"),\n",
    ")\n",
    "df_all_titles_content.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En annen åpenbar kobling er `(tittel, ingress)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_title_ingres = (\n",
    "    df.filter(pl.col(\"ingress\").str.len_bytes() > 0)\n",
    "    .select(\n",
    "        pl.col(\"path\"),\n",
    "        pl.col(\"display_name\").alias(\"anchor\"),\n",
    "        pl.col(\"ingress\").alias(\"positiv\"),\n",
    "    )\n",
    "    .unique()\n",
    ")\n",
    "df_title_ingres.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La oss koble alle disse tabellene sammen og for å lage et endelig datasett."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pl.concat([df_title_ingres, df_all_titles_content])\n",
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rense datasett\n",
    "\n",
    "Før vi sier oss fornøyd skal vi vaske dataene våre litt. I tabellene over har du\n",
    "kanskje lagt merke til at flere av kolonnene inneholder HTML elementer. Det er\n",
    "ikke i utgangspunktet noe galt å bruke dette for trening, men siden vi her\n",
    "fokuserer på en embeddingmodell ønsker vi at den fokuserer på semantikken og\n",
    "ikke formatet. Vi skal derfor prøve å renske bort alle HTML tag-er^[Vi gjør det\n",
    "enkelt med en `regex` inspirert av\n",
    "[StackOverflow](https://stackoverflow.com/a/12982689).]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = df_train.with_columns(\n",
    "    pl.col(\"anchor\").str.replace_all(\"<.*?>\", \" \").str.strip_chars(),\n",
    "    pl.col(\"positiv\").str.replace_all(\"<.*?>\", \" \").str.strip_chars(),\n",
    ")\n",
    "df_train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lagre til fil\n",
    "\n",
    "La oss avslutte med å lagre data til en fil slik at vi enkelt kan gjenskape\n",
    "treningen og samtidig dele data med andre på en enkel måte. Et format som kan\n",
    "være praktisk er [Parquet](https://en.wikipedia.org/wiki/Apache_Parquet) som\n",
    "både er effektivt for å lagre dataframe data og samtidig er godt støttet i de\n",
    "fleste verktøy vi bruker i Nav."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.write_parquet(\"dataset.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trening og test\n",
    "\n",
    "Nå som vi har laget et fullstendig treningssett kan vi begynne å tenke på å dele\n",
    "opp i en trenings del og en test del. Dette gjør vi for å ha en del som modellen\n",
    "får lov til å se på, trenings delen, og en del som er helt ny for modellen, test\n",
    "delen. Ved å skille slik får vi mulighet til å evaluere hvor godt modellen\n",
    "fungerer på ting den ikke har sett før.\n",
    "\n",
    "Vi starter med å legge til en `ID` kolonne på datasettet vårt slik at vi kan\n",
    "unikt identifisere rader, dette kommer vi til å trenge senere.\n",
    "\n",
    "::: {.column-margin}\n",
    "Vi leser inn datasettet på nytt slik at man ikke trenger å kjøre alle cellene\n",
    "hver gang man ønsker å gjøre en endring på oppsettet for språkmodellen.\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "\n",
    "dataset = pl.read_parquet(\"dataset.parquet\").with_row_index(\"id\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter deler vi datasettet i en del for trening, meste parten, og en del for\n",
    "testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Litt komplisert å lage trening/test split i Polars\n",
    "#\n",
    "# Vi starter med å randomisere hele datasettet\n",
    "dataset = dataset.sample(fraction=1, shuffle=True, seed=12345)\n",
    "# Beregne antall rader vi skal bruke\n",
    "num_test = int(0.2 * len(dataset))\n",
    "# Deretter ta de første `num_test` radene til test\n",
    "test_dataset = dataset.head(num_test)\n",
    "# Tilslutt tar vi alle utenom de første `num_test` radene til trening\n",
    "train_dataset = dataset.tail(-num_test)\n",
    "# Skriv ut raske tall\n",
    "print(f\"Antall elementer i trening:\\t{len(train_dataset)}\")\n",
    "print(f\"Antall elementer i test:\\t{len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Corpus\n",
    "\n",
    "Nå som vi har opprettet et datasett kan vi bruke dette for å lage oss et corpus\n",
    "å trene på/med.\n",
    "\n",
    "Vi starter med å lage oss et sett med alt innhold, \"corpus\", og et sett med\n",
    "\"queries\" (de elementene som vi ønsker å teste mot innholdet)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merk at vi bruker `dataset` for å bruke _alt_ innhold\n",
    "corpus = dict(dataset.select([\"id\", \"positiv\"]).rows())\n",
    "# For \"queries\" bruker vi det vi har plukket ut i test\n",
    "queries = dict(test_dataset.select([\"id\", \"anchor\"]).rows())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vi trenger så å lage oss en mapping mellom \"queries\" og relevant innhold. I vårt\n",
    "tilfellet så vil det være overlapp for alle deler som kommer fra samme sti på\n",
    "Nav.no. Sagt på en annen måte, vi ønsker å vekte elementer fra samme sti høyere\n",
    "under evaluering slik at et element fra `/dagpenger` er viktigere eller bedre\n",
    "enn et element fra `/sykepenger` hvis søket handler om Dagpenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant_docs = {}\n",
    "for qid in queries.keys():\n",
    "    navno_path = (\n",
    "        dataset.filter(pl.col(\"id\") == qid).unique(\"path\").item(row=0, column=\"path\")\n",
    "    )\n",
    "    relevant_docs[qid] = set(\n",
    "        dataset.filter(pl.col(\"path\") == navno_path).get_column(\"id\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Språkmodell\n",
    "\n",
    "Nå som vi har ordnet oss med litt data er det endelig på tide å velge en\n",
    "språkmodell. Vi kommer til å bruke\n",
    "[`sentence-transformers`](https://sbert.net/index.html) for modellen og trening\n",
    "så la oss først ordne nødvendige pakker.\n",
    "\n",
    ":::: {.callout-note}\n",
    "## Nødvendige pakker\n",
    "\n",
    "Vi trenger et par pakker for `sentence-transformers` og de avhenger av riktig\n",
    "oppsett for effektiv trening.\n",
    "\n",
    "::: {.panel-tabset}\n",
    "## Uten CUDA (Linux, Mac og Windows uten GPU)\n",
    "For maskiner uten dedikert Nvidia GPU kan man enkelt installere som følger:\n",
    "\n",
    "```bash\n",
    "uv add transformers --extra torch\n",
    "uv add sentence-transformers --extra train\n",
    "```\n",
    "\n",
    "## CUDA\n",
    "\n",
    "Hvis du har et dedikert grafikkort kan du tjene mye på å installere PyTorch med\n",
    "CUDA støtte.\n",
    "\n",
    "Her anbefaler vi å følge oppskriften på [PyTorch sin\n",
    "hjemmeside](https://pytorch.org/get-started/locally/) for å få riktig oppsett\n",
    "for akkurat din maskin.\n",
    "\n",
    "Deretter trenger du:\n",
    "```bash\n",
    "uv add transformers\n",
    "uv add sentence-transformers --extra train\n",
    "```\n",
    ":::\n",
    "\n",
    "::::\n",
    "\n",
    "---\n",
    "\n",
    "Når det kommer til valg av språkmodell så er det vanskelig å gi noen konkrete\n",
    "anbefalinger, nettopp fordi man kan tilpasse modellene til egne data slik vi\n",
    "gjør her. En god oversikt over hvordan å velge språkmodell finnes i [Nav sin\n",
    "tekniske\n",
    "veileder](https://data.ansatt.nav.no/quarto/b9ec1385-d596-47e2-a3d2-8cbc85c577a3/_book/llm/lokale.html).\n",
    "\n",
    "Vi kommer til å gå videre med\n",
    "[`Alibaba-NLP/gte-modernbert-base`](https://huggingface.co/Alibaba-NLP/gte-modernbert-base).\n",
    "Denne har vi valgt av følgende grunner:\n",
    "\n",
    "- Den gjør det godt i sammenligninger mot embeddingmodeller av tilsvarende størrelse\n",
    "- Det er en relativt liten, $149$ millioner parametere, modell som burde passe\n",
    "fint på en laptop\n",
    "- Den har et stort kontekstvindu på $8192$ token\n",
    "- Den er lisensiert på en måte som gjør at vi enkelt kan ta den i bruk i Nav\n",
    "(`Apache 2.0`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "model = SentenceTransformer(\n",
    "    \"Alibaba-NLP/gte-modernbert-base\",\n",
    "    # Merk at her velger vi `device` spesifikt tilpasset Mac, her burde man\n",
    "    # bruke \"cuda\" hvis tilgjengelig, eller annet tilpasset din maskin\n",
    "    device=\"mps\" if torch.backends.mps.is_available() else \"cpu\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluere språkmodell\n",
    "\n",
    "La oss nå se litt på hvordan modellen vår gjør det på datasettet vårt.\n",
    "\n",
    "Vi må starte med å definere en måte å evaluere modellen vår, her har også\n",
    "[`sentence-transformers` god\n",
    "støtte](https://sbert.net/docs/sentence_transformer/training_overview.html#evaluator)\n",
    "så vi benytter det som er innebygget der."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers.evaluation import InformationRetrievalEvaluator\n",
    "from sentence_transformers.util import cos_sim\n",
    "\n",
    "evaluator = InformationRetrievalEvaluator(\n",
    "    queries=queries,\n",
    "    corpus=corpus,\n",
    "    relevant_docs=relevant_docs,\n",
    "    name=\"modernbert\",\n",
    "    score_functions={\"cosine\": cos_sim},\n",
    "    batch_size=4,  # Skru denne ned eller opp avhengig av tilgjengelig minne\n",
    "    show_progress_bar=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deretter kan vi benytte `evaluator` til å vurdere modellene vår."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | eval: false\n",
    "base_eval = evaluator(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fra evalueringen over er det kanskje mest interessant å se på\n",
    "[NDCG@10](https://en.wikipedia.org/wiki/Discounted_cumulative_gain) som sier oss\n",
    "noe om kvaliteten på rangering av treff."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Rangeringskvalitet (NDCG@10): {base_eval['modernbert_cosine_ndcg@10']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
