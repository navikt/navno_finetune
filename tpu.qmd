# Oppsett av Cloud TPU VM

Når vi jobber med språkmodeller er dette ofte såpass store modeller at laptopene
våre kan trenge litt ekstra hjelp for å kunne finjustere. Vi skal her beskrive
hvordan du kan få tilgang til maskinvare på GCP som er spesiallaget for nettopp
å trene maskinlæringsmodeller (så fremt disse modellene er nevrale nettverk).

[_TPU_](https://en.wikipedia.org/wiki/Tensor_Processing_Unit) står for
**T**ensor **P**rocessing **U**nit og er et akseleratorkort spesielt utviklet av
Google for å trene store nevrale nettverk. Som ansatt i Nav kan vi få tilgang
til slik maskinvare gjennom [GCP](https://console.cloud.google.com/).

::: {.callout-warning}
## Bruk av TPU må ROS-es!

Det er dessverre ikke tilgjengelige maskiner med TPU i GCP regionen som Nav har
ROS for. Dette betyr at all bruk av TPU maskinvare foregår på eget ansvar. Team
som ønsker å bruke Cloud TPU VM må ha med bruken av TPU i egen ROS.
:::

## Forberedelser

Det første man må passe på er at man har installert
[`gcloud`](https://cloud.google.com/sdk/docs/install-sdk) og er autentisert mot
et prosjekt.

::: {.callout-tip collapse="true"}
## Sette GCP prosjekt

Hvis du ikke allerede har gjort det, kan det være lurt å definere et standard
prosjekt slik at `gcloud` alltid bruker dette prosjektet.

```bash
gcloud config set project <Prosjekt ID>
```

Hvor `<Prosjekt ID>` er prosjektet du ønsker å bruke som standard.

Hvis du er usikker på hvor du finner dette står det øverst til venstre når du
logger inn på [Google Cloud](https://console.cloud.google.com).
:::

For å autentisere mot Google Cloud og gjør vi med følgende:

```bash
gcloud auth login
```

## Provisjonere en VM

Det første vi trenger å gjøre er å [provisjonere en Cloud TPU
VM](https://console.cloud.google.com/compute/tpus?inv=1&invt=AbrYZQ).

```bash
gcloud compute tpus tpu-vm create <Navn på VM> \
    --project=<Prosjekt ID> \
    --zone=europe-west4-a \
    --accelerator-type=v6e-1 \
    --version=v2-alpha-tpuv6e \
    --tags=allow-ssh
```

::: {.callout collapse="true"}
## Forklaring på variabler

- `<Navn på VM>`
    - Eget valgt navn på maskinen
- `<Project ID>`
    - Ditt GCP prosjekt
- `--zone`
    - Hvor skal maskinen provisjoneres
    - I Nav bruker vi _vanligvis_ `europe-north1`. Det er dessverre ikke
    tilgjengelige TPU i `europe-north1` så vi må bruke [en
    annen](https://cloud.google.com/tpu/docs/regions-zones)
- `--accelerator-type`
    - Hvilken generasjon av TPU skal brukes, her kan
    [pris](https://cloud.google.com/tpu?hl=en#pricing) være førende
- `--version`
    - Hvilken versjon av operativsystemet ønsker vi, henger sammen med hvilken
    TPU generasjon man velger
:::

Når du kjører denne kommandoen for første gang kan man bli spurt om man ønsker å
skru på API `[tpu.googleapis.com]`, dette er helt trygt og man kan svare `y`.

## Koble til VM

Når maskinen er provisjonert kan vi koble til med SSH gjennom `gcloud`.

```bash
gcloud compute tpus tpu-vm ssh <Navn på VM> \
    --project=<Prosjekt ID> \
    --zone=europe-west4-a
```

### Koble til med VS Code

For å kunne koble til med VS Code Remote SSH følger vi noen av stegene fra
[Knast sitt
oppsett](https://docs.knada.io/analyse/knast/kom-i-gang/#flgende-ma-gjres-pa-lokal-maskin-for-a-koble-vs-code-til-knast).

1. Opprett SSH-nøkkel som beskrevet i Knast oppsettet (steg 4)
1. Kopier `.pub` delen til TPU VM (steg 5)
1. Legg til TPU VM i din egen `.ssh/config`:
```bash
Host tpuv6e-1
    User user
    Hostname <External IP>
```
- Du finner **External IP** på [TPU oversikten på Google
Cloud](https://console.cloud.google.com/compute/tpus)

::: {.callout-note collapse="true"}
## IP vil forandre seg

Når TPU VM-en startes og stoppes vil `External IP` endre seg. Man vil derfor
måtte gjøre oppdateringer i SSH konfigurasjon.
:::

Tilslutt er det bare å følge steg 7 på Knast veiledningen for å få VS Code til å
koble til TPU VM-en.

::: {.callout-tip}
## Jupyter notebook på Remote SSH

For en bedre opplevelse med remote SSH kan det være greit å installere følgende
pakker:

```bash
uv add --dev ipykernel ipywidgets
```
:::

## Gjøre klar VM til finjustering

Etter at man har koblet seg til VM-en er det nesten klart for å starte
finjustering. Først må vi bare gjøre klart for Python og nødvendige pakker for
TPU-en.

Først trenger vi å installere [`uv`](https://docs.astral.sh/uv/) (som vi skal
bruke som prosjektverktøy).

```bash
curl -LsSf https://astral.sh/uv/install.sh | sh
```

Deretter skal vi oppdatere programvare på VM-en og installere en nødvendig
ekstern avhengighet.

```bash
sudo apt-get update
sudo apt-get install -y libopenblas-dev
```

Deretter kan vi opprette et prosjekt på TPU VM-en vår.

```bash
uv init --app --python 3.11 navno_finetune
```

::: {.callout-tip collapse="true"}
## Gammel versjon av Python?

`torch_xla` støtter ikke Python 3.12 eller 3.13 enda så vi må forholde oss
til 3.11 enn så lenge. Heldigvis er dette veldig enkelt med `uv`.
:::

Rediger `pyproject.toml` og erstatt `dependencies` med følgende:

```toml
dependencies = [
    "torch>=2.6.0",
    "torch_xla[tpu]>=2.6.0",
]

[tool.uv]
find-links = [
    "https://storage.googleapis.com/libtpu-wheels/index.html",
    "https://storage.googleapis.com/libtpu-releases/index.html",
]
prerelease = "allow"
```

Hopp inn i prosjektet og kjør:

```bash
cd navno_finetune/
uv sync
```

## Stoppe VM når man er ferdig

En Cloud TPU VM koster penger per time den er aktivert (på samme måte som
Knast), så det er en god ide å stoppe den når man ikke ønsker å bruke den mer
(på Knast skjer dette automagisk, men dessverre ikke med TPU VM).

For å stoppe VM-en kan man kjøre følgende:

```bash
gcloud compute tpus tpu-vm stop <Navn på VM> --zone=europe-west4-a
```

Når VM-en er stoppet vil det ikke koste prosjektet noe, men man vil kunne
beholde oppsett på VM-en.

### Start VM for å fortsette

Når man så er klar for å fortsette å arbeide kan man start VM-en igjen med:

```bash
gcloud compute tpus tpu-vm start <Navn på VM> --zone=europe-west4-a
```

::: {.callout-note collapse="true"}
## IP vil forandre seg

Når TPU VM-en startes og stoppes vil `External IP` endre seg. Man vil derfor
måtte gjøre oppdateringer i SSH konfigurasjon.
:::

### Slette VM

Hvis du vet at du ikke skal bruke VM-en igjen (eller tenker at man kan opprette
en helt ny VM) kan man slette VM-en helt.

Følgende kommando sletter VM fullstendig:

```bash
gcloud compute tpus tpu-vm delete <Navn på VM> --zone=europe-west4-a --quiet
```

## Tips og triks

### Utnyttelse av TPU

Når man installerer `torch_xla[tpu]` vil man også installere
[`tpu-info`](https://pypi.org/project/tpu-info/) som standard. Dette er et nytte
verktøy som kan vise utnyttelsen av TPU (f.eks. hvis man er usikker på om
modellen kjører på TPU-en).

```bash
uv run tpu-info
```

::: {.callout-tip collapse="true"}
## Hvis `tpu-info` ikke er installert

Med `uv` er det enkelt å kjøre Python programmer som ikke er installert. Skulle
`uv run` kommando-en over feile kan du alltids gjøre det samme med:

```bash
uvx tpu-info
```

Som vil laste ned `tpu-info` om det ikke er installert.
:::
